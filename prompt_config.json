{
  "evaluation": {
    "agent": {
      "system_message": "You are a workplace assistant that can use tools (Slack, Google Contacts, Google Calendar, Jira, Gmail, Google Drive) to answer user requests. You are evaluated on correct tool use and reasoning. When you are ready to give your final answer, you MUST respond with a single JSON object of the form {\"final_answer\": \"...\", \"rationale\": \"...\"} and nothing else."
    },
    "judge": {
      "system_prompt": "You are an evaluation model for a multi-source workplace benchmark. You will receive a single JSON object describing:\n\n- The task and user prompt.\n- A concise list of answer_requirements that encodes the essential constraints and reasoning steps for a good solution.\n- A list of tool trace steps that shows what tools were actually called (human-readable summary).\n- A structured tool_trace array with the full tool queries and results from the recorded execution.\n- The model's final answer.\n- The model's rationale (its own explanation of how it solved the task).\n\nYour job is to score the model along two dimensions:\n\n1) Answer requirements satisfaction (0–5)\n2) Source grounded reasoning (0–5)\n\nIMPORTANT ASSUMPTIONS\n\n- answer_requirements is intentionally concise: it contains only the essential constraints and high-level steps that a good solution should respect.\n- answer_requirements may include both:\n  - required reasoning steps (e.g., 'identify participants → map to emails → check common free time'), and\n  - required factual/structural constraints (e.g., 'meeting time must lie within working hours and within the common free slots', 'release ETA must follow policy').\n- Do not assume that any fact not mentioned in answer_requirements is necessarily false; instead, treat answer_requirements as an authoritative core subset of what matters for this task.\n- Do not hallucinate tool calls, arguments, or data that are not present in the input.\n\nINPUT FORMAT\n\nYou will be given a single JSON object with the following structure:\n\n{\n  \"task_id\": \"...\",\n  \"task_type\": \"planning\" | \"email_reply\" | \"weekly_report\",\n  \"user_prompt\": \"...\",\n  \"answer_requirements\": [\n    \"Step 1 or requirement 1 ...\",\n    \"Step 2 or requirement 2 ...\",\n    ...\n  ],\n  \"tool_trace_steps\": [\n    \"Step 1: ToolName(arg_summary)\",\n    \"Step 2: ToolName(arg_summary)\",\n    ...\n  ],\n  \"raw_tool_calls\": [\n    {\"tool_name\": \"...\", \"arguments\": {...}, \"result\": {...}},\n    ...\n  ],\n  \"tool_trace\": [\n    {\n      \"step\": 1,\n      \"tool_name\": \"ToolName\",\n      \"arguments\": {...},\n      \"result\": {...}\n    },\n    ...\n  ],\n  \"final_answer\": \"...\",\n  \"rationale\": \"...\"\n}\n\nNotes:\n\n- task_id, task_type, user_prompt: identify the task being evaluated.\n- answer_requirements: a short list of essential steps and constraints that a correct solution should conceptually follow and satisfy.\n- tool_trace_steps: a list of human-readable strings that summarize which tools were called and with which arguments. This is a rough, human-readable trace only.\n- raw_tool_calls: a list of objects, each with:\n  - tool_name (the name of the tool that was called),\n  - arguments (the exact arguments used in the call),\n  - result (the full JSON result of the tool call).\n  This contains the full tool queries and results as recorded during execution. Use this field to verify factual consistency with tool outputs.\n- tool_trace: a list of objects, each with:\n  - step (1-based index),\n  - tool_name (the name of the tool that was called),\n  - arguments (the exact arguments used in the call),\n  - result (the full JSON result of the tool call).\n  This is an alternative structured view of the same tool calls. Both raw_tool_calls and tool_trace contain the same information in different formats.\n- final_answer: the model's final answer to the user.\n- rationale: the model's explanation of how it solved the task.\n\nIMPORTANT: You are evaluating based ONLY on the recorded data in the judge input JSON. Do NOT assume or query any external tools. Only use the recorded tool results in raw_tool_calls or tool_trace.\n\nYOUR SCORING TASK\n\nYou must output a single JSON object with the following structure:\n\n{\n  \"answer_requirements_satisfaction\": {\n    \"score\": 0–5 integer,\n    \"justification\": \"short explanation\"\n  },\n  \"source_grounded_reasoning\": {\n    \"score\": 0–5 integer,\n    \"justification\": \"short explanation\"\n  }\n}\n\nDo NOT add any extra top-level fields. Do NOT wrap this JSON in backticks or any other formatting.\n\nDETAILED RUBRICS\n\n1) Answer requirements satisfaction (0–5)\n\nGoal: Does the final_answer (and rationale) satisfy the answer_requirements? Treat this as \"correctness / success\" in the absence of an oracle answer.\n\n- Use answer_requirements as the specification of what a good answer should do.\n- For each requirement sentence, check whether the final_answer (and rationale) satisfies it:\n  - Does the answer produce the required type of output (e.g., concrete meeting time slots)?\n  - Are constraints and conditions respected (e.g., participant set, date range, duration, working hours)?\n- Use wording cues for importance:\n  - Phrases like \"must\", \"must not\", \"required to\", \"cannot\" indicate a stronger / more critical requirement.\n  - Phrases like \"if possible\", \"preferably\", \"ideally\", \"it is better to\" indicate softer preferences.\n  - You may treat \"should\" as intermediate; when it concerns safety/privacy/core constraints, treat it closer to a hard requirement.\n\nScoring guidelines:\n\n- 5: All important requirements (strong wording) are satisfied; soft preferences are largely respected.\n- 3–4: Most important requirements satisfied, but some requirements or soft preferences are missing or only partially addressed.\n- 1–2: Multiple important requirements are violated or missing; the answer conflicts with key constraints.\n- 0: The answer is largely off-spec (wrong type/format or ignores almost all requirements).\n\n2) Source grounded reasoning (0–5)\n\nGoal: Evaluate how well the rationale and final_answer are grounded in the tool/source calls and their results.\n\n- Use raw_tool_calls (or tool_trace) as the PRIMARY SOURCE OF TRUTH about what the tools actually returned.\n- When deciding whether the final answer and rationale are faithful to the tools, compare them against:\n  - raw_tool_calls[*].result or tool_trace[*].result (for factual consistency with tool outputs), and\n  - the sequence of tool calls given by raw_tool_calls / tool_trace / tool_trace_steps (for process consistency).\n- Do NOT assume or query any external tools; only use the recorded tool results in raw_tool_calls or tool_trace.\n\nCheck two aspects together in this single axis:\n\n  (a) Source result factual faithfulness\n  - The answer/rationale should not invent facts that contradict the tool results in raw_tool_calls[*].result (or tool_trace[*].result).\n  - Compare claims in the final_answer and rationale directly against the actual tool results:\n    - Do not claim a free time slot that is not supported by the calendar results in raw_tool_calls.\n    - Do not attribute messages to Slack that do not appear in the Slack results in raw_tool_calls.\n    - Do not change numerical values, times, or key entities from what the tools returned in raw_tool_calls.\n    - Pay special attention to date consistency: if the user prompt mentions a time period (e.g., \"next week\"), verify that the dates in the final_answer match the dates in the tool results. If the tool was called with incorrect dates (e.g., 2023 when it should be 2025), or if the final_answer uses dates that don't match the tool results, this is a serious error that should significantly lower the score.\n  - Small paraphrases are fine; fabricating or distorting important details is not.\n\n  (b) Consistency with the tool trace (process description)\n  - The rationale's description of \"what tools were used and why\" should match raw_tool_calls / tool_trace / tool_trace_steps.\n  - Compare the rationale's process description against the sequence in raw_tool_calls:\n    - Do not claim to have used tools that were never called (check raw_tool_calls[*].tool_name).\n    - Do not invent an entirely different sequence of steps than what raw_tool_calls shows.\n  - It is okay to omit minor details, but the main story of the process should match.\n\nScoring guidelines:\n\n- 5: The explanation is strongly aligned with both the tool results (from raw_tool_calls[*].result) and the tool trace sequence; no major fabrications or process inconsistencies.\n- 3–4: Mostly grounded and correct, but with some minor inaccuracies or omissions in how tool results or steps are described relative to raw_tool_calls.\n- 1–2: Several mismatches between the explanation and the tool results/trace (as recorded in raw_tool_calls); noticeable invented details or misleading process description.\n- 0: The reasoning largely ignores the tools, contradicts their results (as shown in raw_tool_calls), or invents an incompatible process.\n\nOUTPUT REQUIREMENTS\n\n- Return ONLY a JSON object with the following structure (no extra keys, no prose outside the JSON):\n\n  {\n    \"answer_requirements_satisfaction\": {\n      \"score\": <integer 0-5>,\n      \"justification\": \"<short explanation>\"\n    },\n    \"source_grounded_reasoning\": {\n      \"score\": <integer 0-5>,\n      \"justification\": \"<short explanation>\"\n    }\n  }\n\n- Do NOT include any extra commentary outside the JSON.\n- Do NOT use markdown, code fences, or backticks."
    }
  },
  "generator": {
    "world_state": {
      "system_prompt": "You are a world state generator for a workplace benchmark. Your job is to transform an abstract scenario into a concrete, detailed world_state JSON.\n\nThe world_state should include:\n1. All metadata from the scenario (scenario_id, description, noise_level, depth, global_settings, people, projects)\n2. sub_scenarios_expanded: For each abstract sub_scenario from the input scenario, create a concrete expanded version with:\n   - Concrete ISO datetime timestamps\n   - Specific participant IDs/emails\n   - Detailed event descriptions\n   - Any seeds needed for data generation\n   - This array should contain ONLY the expanded versions of the sub_scenarios listed in the input scenario\n3. noise_scenarios: An array of noise-only scenario objects that are NOT part of the core sub_scenarios:\n   - These should be realistic but unrelated workplace events/messages/activities\n   - Each noise scenario should have similar structure to sub_scenarios_expanded (id, type, timestamps, participants, etc.)\n   - The number of noise scenarios should scale with the noise_level parameter\n\nIMPORTANT: Do NOT include per_source_plans in the world_state. The world_state is scenario-centric only.\n\nIMPORTANT PARAMETERS:\n- noise_level (0-1): Controls amount of unrelated/off-task data\n  * 0 = almost no noise, only data directly relevant to sub_scenarios (noise_scenarios should be empty or minimal)\n  * 1 = lots of unrelated messages/events/files (still realistic) (noise_scenarios should contain many entries)\n- depth (0-1): Controls how indirect the evidence is\n  * 0 = sub_scenario realized directly (e.g., one clear Slack message)\n  * 1 = sub_scenario requires multi-step, multi-source chaining to infer\n  * Intermediate values interpolate behavior\n\nCRITICAL: sub_scenarios_expanded must correspond exactly to the sub_scenarios in the input scenario. All other unrelated scenarios should go in noise_scenarios.\n\nYou must output ONLY valid JSON matching the world_state schema. Do not include markdown code fences.",
      "user_prompt_template": "Generate a world_state JSON from this scenario:\n\n{scenario_json}\n\nRemember:\n- noise_level={noise_level}: {noise_level_desc}\n- depth={depth}: {depth_desc}\n\nOutput the complete world_state JSON now."
    },
    "contacts": {
      "system_prompt": "You are a Contacts data generator for a workplace benchmark. Your job is to generate realistic contact data based on scenario information.\n\nThe output must be valid JSON matching the Contacts schema with:\n- contacts: array of {id, name, email, phone (optional), organization (optional)}\n\nGenerate contacts that include:\n- All people from the world_state (required)\n- Any additional contacts mentioned in sub_scenarios_expanded or noise_scenarios\n- Ensure all participant IDs from scenarios have corresponding contacts\n\nYou must output ONLY valid JSON matching the schema. Do not include markdown code fences.",
      "user_prompt_template": "Generate Contacts data from this world_state:\n\nPeople (must all be included as contacts):\n{people_json}\n\nSub-scenarios (may reference additional participants):\n{sub_scenarios_expanded_json}\n\nNoise scenarios (may reference additional participants):\n{noise_scenarios_json}\n\nTarget schema:\n{contacts_schema_json}\n\nOutput the complete Contacts JSON now."
    },
    "calendar": {
      "system_prompt": "You are a Calendar data generator for a workplace benchmark. Your job is to generate realistic calendar event data based on scenario information.\n\nThe output must be valid JSON matching the Calendar schema with:\n- events: array of {id, title, start, end, attendees[]}\n- calendars: array of {id, email} (one per person)\n\nGenerate calendar events that reflect:\n- Events from sub_scenarios_expanded (core scenario events, especially meetings)\n- Events from noise_scenarios (unrelated workplace meetings/events)\n- Respect noise_level: 0 = minimal noise, 1 = lots of unrelated events\n- Respect depth: 0 = direct/obvious events, 1 = indirect/multi-step scheduling\n\nYou must output ONLY valid JSON matching the schema. Do not include markdown code fences.",
      "user_prompt_template": "Generate Calendar event data from this world_state:\n\nSub-scenarios (core events):\n{sub_scenarios_expanded_json}\n\nNoise scenarios (unrelated events):\n{noise_scenarios_json}\n\nPeople:\n{people_json}\n\nGlobal settings:\n{global_settings_json}\n\nParameters:\n- noise_level={noise_level}: {noise_level_desc}\n- depth={depth}: {depth_desc}\n\nTarget schema:\n{calendar_schema_json}\n\nOutput the complete Calendar JSON now."
    },
    "slack": {
      "system_prompt": "You are a Slack data generator for a workplace benchmark. Your job is to generate realistic Slack workspace data (channels, messages, users) based on scenario information.\n\nThe output must be valid JSON matching the Slack schema with:\n- workspace_name: string\n- channels: array of {id, name}\n- messages: array of {id, channel_id, user_id, text, timestamp}\n- users: array of {id, name, email}\n\nGenerate messages that reflect:\n- Events from sub_scenarios_expanded (core scenario events)\n- Events from noise_scenarios (unrelated workplace activity)\n- Respect noise_level: 0 = minimal noise, 1 = lots of unrelated messages\n- Respect depth: 0 = direct/obvious messages, 1 = indirect/multi-step conversations\n\nYou must output ONLY valid JSON matching the schema. Do not include markdown code fences.",
      "user_prompt_template": "Generate Slack workspace data from this world_state:\n\nSub-scenarios (core events):\n{sub_scenarios_expanded_json}\n\nNoise scenarios (unrelated events):\n{noise_scenarios_json}\n\nPeople:\n{people_json}\n\nParameters:\n- noise_level={noise_level}: {noise_level_desc}\n- depth={depth}: {depth_desc}\n\nTarget schema:\n{slack_schema_json}\n\nOutput the complete Slack JSON now."
    },
    "gmail": {
      "system_prompt": "You are a Gmail data generator for a workplace benchmark. Your job is to generate realistic email thread data based on scenario information.\n\nThe output must be valid JSON matching the Gmail schema with:\n- threads: array of {id, subject, messages[]}\n  - messages: array of {id, from, to[], subject, body, date}\n\nGenerate email threads that reflect:\n- Events from sub_scenarios_expanded (core scenario events)\n- Events from noise_scenarios (unrelated workplace activity)\n- Respect noise_level: 0 = minimal noise, 1 = lots of unrelated emails\n- Respect depth: 0 = direct/obvious emails, 1 = indirect/multi-step conversations\n\nYou must output ONLY valid JSON matching the schema. Do not include markdown code fences.",
      "user_prompt_template": "Generate Gmail thread data from this world_state:\n\nSub-scenarios (core events):\n{sub_scenarios_expanded_json}\n\nNoise scenarios (unrelated events):\n{noise_scenarios_json}\n\nPeople:\n{people_json}\n\nParameters:\n- noise_level={noise_level}: {noise_level_desc}\n- depth={depth}: {depth_desc}\n\nTarget schema:\n{gmail_schema_json}\n\nOutput the complete Gmail JSON now."
    },
    "jira": {
      "system_prompt": "You are a Jira data generator for a workplace benchmark. Your job is to generate realistic Jira issue data based on scenario information.\n\nThe output must be valid JSON matching the Jira schema with:\n- projects: array of {key, name, fixVersions[]}\n- issues: array of {key, summary, status, updated (optional), fixVersions[] (optional), project: {key}}\n\nGenerate Jira issues that reflect:\n- Seed issues from projects.jira_seeds (must be included)\n- Events from sub_scenarios_expanded (core scenario events related to issues)\n- Events from noise_scenarios (unrelated workplace issues)\n- Respect noise_level: 0 = minimal noise, 1 = lots of unrelated issues\n- Respect depth: 0 = direct/obvious issue updates, 1 = indirect/multi-step issue tracking\n\nYou must output ONLY valid JSON matching the schema. Do not include markdown code fences.",
      "user_prompt_template": "Generate Jira issue data from this world_state:\n\nProjects (with jira_seeds that must be included):\n{projects_json}\n\nSub-scenarios (core events):\n{sub_scenarios_expanded_json}\n\nNoise scenarios (unrelated events):\n{noise_scenarios_json}\n\nParameters:\n- noise_level={noise_level}: {noise_level_desc}\n- depth={depth}: {depth_desc}\n\nTarget schema:\n{jira_schema_json}\n\nOutput the complete Jira JSON now."
    },
    "drive": {
      "system_prompt": "You are a Google Drive data generator for a workplace benchmark. Your job is to generate realistic Drive file data based on scenario information.\n\nThe output must be valid JSON matching the Drive schema with:\n- files: array of {id, name, mimeType (optional), content (optional text excerpt), modifiedTime (optional)}\n\nGenerate Drive files that reflect:\n- Events from sub_scenarios_expanded (core scenario events, e.g., documents, reports)\n- Events from noise_scenarios (unrelated workplace files)\n- Respect noise_level: 0 = minimal noise, 1 = lots of unrelated files\n- Respect depth: 0 = direct/obvious files, 1 = indirect/multi-step file references\n\nYou must output ONLY valid JSON matching the schema. Do not include markdown code fences.",
      "user_prompt_template": "Generate Drive file data from this world_state:\n\nSub-scenarios (core events):\n{sub_scenarios_expanded_json}\n\nNoise scenarios (unrelated events):\n{noise_scenarios_json}\n\nProjects (may reference project-related files):\n{projects_json}\n\nParameters:\n- noise_level={noise_level}: {noise_level_desc}\n- depth={depth}: {depth_desc}\n\nTarget schema:\n{drive_schema_json}\n\nOutput the complete Drive JSON now."
    }
  }
}
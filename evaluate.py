"""Evaluation runner for MPCBench v2.

Runs evaluation over many tasks, computes scores, and writes logs.
"""

from typing import Dict, Any, List
from pathlib import Path
import json

from config import LOGS_DIR, MODEL_CONFIG_PATH
from task_defs import load_all_tasks, Task, get_planning_meeting_slots
from data_gen import generate_source_data
from agent_runner import run_task


def score_planning_answer(task: Task, agent_answer_text: str) -> float:
    """
    [Core function] Score agent's answer for a planning task
    
    Purpose: Evaluate how well the agent's proposed time slots match the canonical answer
    
    Scoring method:
        1. Perfect match (1.0): All canonical slots included + no extra slots
        2. Partial match: Some canonical slots included → calculate ratio
        3. Penalty: Extra slots result in score deduction (max 50% deduction)
        4. Final score = (matched_canonical / total_canonical) * (1.0 - penalty * 0.5)
    
    Example:
        canonical: [(2025-12-02, 14:00-14:45), (2025-12-03, 13:00-13:45)]
        agent: [(2025-12-02, 14:00-14:45), (2025-12-03, 13:00-13:45), (2025-12-02, 15:00-15:45)]
        → matched=2, extra=1 → base_score=1.0, penalty=1/3 → score=1.0*(1-0.33*0.5)=0.835
    
    Args:
        task: Task definition (includes canonical_answer)
        agent_answer_text: Answer text generated by the agent
        
    Returns:
        Score between 0.0 and 1.0
    """
    # Return 0 if not a planning task
    if task.category != "planning":
        return 0.0
    
    # Extract canonical slots (ground truth slots)
    slots = get_planning_meeting_slots(task)
    if not slots:
        return 0.0
    
    # ============================================================
    # Step 1: Extract date-time pairs from agent answer
    # ============================================================
    # Agent may express dates/times in various formats
    # Extract using multiple patterns and normalize
    
    answer_lower = agent_answer_text.lower()
    import re
    
    # Date patterns (support various formats)
    date_patterns = [
        r'(\d{4}-\d{2}-\d{2})',  # 2025-12-02
        r'((?:january|february|...)\s+\d{1,2},?\s+\d{4})',  # December 2, 2025
        r'((?:jan|feb|...)\s+\d{1,2},?\s+\d{4})',  # Dec 2, 2025
        r'(\d{1,2}[/-]\d{1,2}[/-]\d{4})',  # 12/02/2025
    ]
    
    # Time patterns (start-end time range)
    time_patterns = [
        r'(\d{1,2}:\d{2}\s*[-–—to]+\s*\d{1,2}:\d{2})',  # 14:00 - 14:45
    ]
    
    # Extract all dates and times from answer
    found_dates = []
    found_times = []
    
    for pattern in date_patterns:
        for match in re.finditer(pattern, answer_lower):
            found_dates.append((match.start(), match.group(1)))  # (position, date string)
    
    for pattern in time_patterns:
        for match in re.finditer(pattern, answer_lower):
            found_times.append((match.start(), match.group(1)))  # (position, time string)
    
    # Match dates and times into pairs
    # Match times that appear after each date with that date
    date_time_pairs = []
    for i, (date_pos, date_str) in enumerate(found_dates):
        # Range until next date
        next_date_pos = found_dates[i+1][0] if i+1 < len(found_dates) else len(answer_lower)
        # Match all times within this range
        for time_pos, time_str in found_times:
            if date_pos < time_pos < next_date_pos:
                date_time_pairs.append((date_str, time_str))
    
    # If no structured pairs found, try all date-time combinations (less strict matching)
    if not date_time_pairs and found_dates and found_times:
        for date_str, _ in found_dates:
            for _, time_str in found_times:
                date_time_pairs.append((date_str, time_str))
    
    # ============================================================
    # Step 2: Normalize date/time formats
    # ============================================================
    # Convert various date/time formats to canonical format (YYYY-MM-DD, HH:MM-HH:MM)
    
    def normalize_date(date_str):
        """Convert various date formats to YYYY-MM-DD"""
        import datetime
        formats = [
            "%Y-%m-%d",  # 2025-12-02
            "%B %d, %Y",  # December 2, 2025
            "%b %d, %Y",  # Dec 2, 2025
            "%m/%d/%Y",  # 12/02/2025
            "%d/%m/%Y",  # 02/12/2025
            "%m-%d-%Y",  # 12-02-2025
        ]
        for fmt in formats:
            try:
                dt = datetime.datetime.strptime(date_str, fmt)
                return dt.strftime("%Y-%m-%d")
            except:
                continue
        return None
    
    def normalize_time(time_str):
        """Convert various time formats to HH:MM-HH:MM"""
        time_str_clean = re.sub(r'\s+', '', time_str.lower())
        # Extract both start and end times
        times = re.findall(r'(\d{1,2}):(\d{2})', time_str_clean)
        if len(times) >= 2:
            start_h, start_m = times[0]
            end_h, end_m = times[1]
            return f"{int(start_h):02d}:{start_m}-{int(end_h):02d}:{end_m}"
        return None
    
    # Normalize agent's mentioned slots into a set
    agent_slots = set()
    for date_str, time_str in date_time_pairs:
        norm_date = normalize_date(date_str)
        norm_time = normalize_time(time_str)
        if norm_date and norm_time:
            agent_slots.add((norm_date, norm_time))
    
    # Convert canonical slots to the same format
    canonical_slots = {(slot["date"], slot["slot"]) for slot in slots}
    
    # ============================================================
    # Step 3: Calculate score
    # ============================================================
    matched_canonical = len(agent_slots & canonical_slots)  # Number of matching canonical slots
    extra_slots = len(agent_slots - canonical_slots)  # Number of extra slots proposed by agent
    missing_canonical = len(canonical_slots - agent_slots)  # Number of missing canonical slots
    
    # Perfect match: all canonical included + no extra slots
    if matched_canonical == len(canonical_slots) and extra_slots == 0:
        return 1.0
    
    # Partial match: some match or extra slots present
    if extra_slots > 0 or missing_canonical > 0:
        # Base score: ratio of matched canonical slots
        base_score = matched_canonical / len(canonical_slots) if len(canonical_slots) > 0 else 0.0
        # Penalty: ratio of extra slots (max 50% deduction)
        penalty = min(extra_slots / max(len(agent_slots), 1), 1.0)
        return max(0.0, base_score * (1.0 - penalty * 0.5))
    
    return 0.0


def evaluate_task(
    task: Task,
    agent_model: str = "gpt-4o-mini",
    generate_data: bool = True,
    tool_context_mode: str = "detailed"
) -> Dict[str, Any]:
    """
    [Core function] Evaluate a single task
    
    Overall flow:
        1. Generate or load source data
        2. Validate data consistency
        3. Run agent (LLM uses tools to generate answer)
        4. Score the answer
        5. Save results
    
    Args:
        task: Task definition to evaluate
        agent_model: LLM model name to use
        generate_data: Whether to generate data if not present
        tool_context_mode: "minimal" or "detailed" (tool description detail level)
        
    Returns:
        Evaluation result dictionary:
        {
            "task_id": ...,
            "agent_model": ...,
            "agent_result": {...},  # final_answer, rationale, tool_calls
            "scores": {
                "answer_requirements_satisfaction": 0.0~1.0
            },
            ...
        }
    """
    # ============================================================
    # Step 1: Generate or load source data
    # ============================================================
    task_data_dir = LOGS_DIR / task.id / "data"
    if generate_data:
        # Generate data (call generate_source_data from data_gen.py)
        source_data = generate_source_data(task, task_data_dir)
    else:
        # Load existing data
        source_data = {}
        source_files = {
            "calendar": task_data_dir / "calendar.json",
            "slack": task_data_dir / "slack.json",
            "jira": task_data_dir / "jira.json",
            "drive": task_data_dir / "drive.json",
            "gmail": task_data_dir / "gmail.json"
        }
        for source_name, file_path in source_files.items():
            if file_path.exists():
                source_data[source_name] = file_path

    # ============================================================
    # Step 2: Run agent
    # ============================================================
    # LLM uses tools to solve the task and generate an answer
    run_log_path = LOGS_DIR / task.id / f"agent-{agent_model}_{tool_context_mode}_run.json"
    agent_result = run_task(task, source_data, agent_model, run_log_path, tool_context_mode=tool_context_mode)

    # ============================================================
    # Step 3: Score the answer
    # ============================================================
    agent_answer_text = agent_result.get("final_answer", "")
    
    # Calculate score by task category
    if task.is_planning():
        answer_score = score_planning_answer(task, agent_answer_text)
    else:
        answer_score = 0.0  # TODO: Implement other categories

    result = {
        "task_id": task.id,
        "agent_model": agent_model,
        "task_category": task.category,
        "metadata": task.metadata,
        "canonical_answer": task.canonical_answer,
        "agent_result": agent_result,
        "scores": {
            "answer_requirements_satisfaction": answer_score
        }
    }

    # Save evaluation result
    eval_log_path = LOGS_DIR / task.id / f"agent-{agent_model}_{tool_context_mode}_eval.json"
    eval_log_path.parent.mkdir(parents=True, exist_ok=True)
    with open(eval_log_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2)

    return result


def evaluate_all_tasks(
    agent_models: List[str] = None,
    generate_data: bool = True,
    tool_context_modes: List[str] = None
) -> Dict[str, Any]:
    """
    [Main function] Orchestration function to evaluate all tasks
    
    Overall flow:
        - Evaluate all combinations of task × agent model × tool_context_mode
        - Call evaluate_task() for each combination
        - Print progress
    
    Args:
        agent_models: List of models to evaluate (None to load from model_config.json)
        generate_data: Whether to generate data if not present
        tool_context_modes: List of tool context modes to evaluate (None for ["minimal", "detailed"])
        
    Returns:
        Dictionary of evaluation results for all task-model-mode combinations
        key: "{task_id}__{model}__{mode}"
    """
    if agent_models is None:
        # Load from model_config.json
        try:
            with open(MODEL_CONFIG_PATH, 'r', encoding='utf-8') as f:
                model_config = json.load(f)
                agent_models = model_config.get("agent_models", ["gpt-4o-mini"])
        except Exception:
            agent_models = ["gpt-4o-mini"]

    if tool_context_modes is None:
        tool_context_modes = ["minimal", "detailed"]
    
    tasks = load_all_tasks()
    results = {}
    
    total_combinations = len(tasks) * len(agent_models) * len(tool_context_modes)
    current = 0

    for task_idx, task in enumerate(tasks, 1):
        for model in agent_models:
            for mode in tool_context_modes:
                current += 1
                key = f"{task.id}__{model}__{mode}"
                print(f"[{mode}] Task {task_idx}/{len(tasks)} ({model}): {task.id}...", end=" ", flush=True)
                try:
                    results[key] = evaluate_task(task, model, generate_data, tool_context_mode=mode)
                    print(f"✓")
                except Exception as e:
                    print(f"⚠️  Error: {e}")
                    # Skip this mode and continue with next
                    continue

    return results

"""Evaluation runner for MPCBench v2.

Runs evaluation over many tasks, computes scores, and writes logs.
"""

from typing import Dict, Any, List
from pathlib import Path
import json

from config import LOGS_DIR, MODEL_CONFIG_PATH
from task_defs import load_all_tasks, Task, get_planning_meeting_slots
from data_gen import generate_source_data
from agent_runner import run_task


def score_planning_answer(task: Task, agent_answer_text: str) -> float:
    """
    [Core function] Score agent's answer for a planning task
    
    Purpose: Evaluate how well the agent's proposed time slots match the canonical answer
    
    Scoring method: F1 Score
        - Precision = matched_canonical / total_agent_slots
          (How many of the agent's proposed slots are correct?)
        - Recall = matched_canonical / total_canonical
          (How many canonical slots did the agent find?)
        - F1 = 2 * (Precision * Recall) / (Precision + Recall)
          (Harmonic mean of Precision and Recall)
    
    This scoring method penalizes both:
        - Missing canonical slots (low Recall)
        - Including distractor slots (low Precision)
    
    Example:
        canonical: [(2025-12-02, 14:00-14:45), (2025-12-03, 13:00-13:45)]
        agent: [(2025-12-02, 14:00-14:45), (2025-12-03, 13:00-13:45), (2025-12-02, 15:00-15:45)]
        → matched=2, total_agent=3, total_canonical=2
        → Precision = 2/3 = 0.67, Recall = 2/2 = 1.0
        → F1 = 2 * (0.67 * 1.0) / (0.67 + 1.0) = 0.80
    
    Args:
        task: Task definition (includes canonical_answer)
        agent_answer_text: Answer text generated by the agent
        
    Returns:
        F1 score between 0.0 and 1.0
    """
    # Return 0 if not a planning task
    if task.category != "planning":
        return 0.0
    
    # Extract canonical slots (ground truth slots)
    slots = get_planning_meeting_slots(task)
    if not slots:
        return 0.0
    
    # ============================================================
    # Step 1: Extract date-time pairs from agent answer
    # ============================================================
    # Agent may express dates/times in various formats
    # Extract using multiple patterns and normalize
    
    answer_lower = agent_answer_text.lower()
    import re
    
    # Date patterns (support various formats)
    date_patterns = [
        r'(\d{4}-\d{2}-\d{2})',  # 2025-12-02
        r'((?:january|february|...)\s+\d{1,2},?\s+\d{4})',  # December 2, 2025
        r'((?:jan|feb|...)\s+\d{1,2},?\s+\d{4})',  # Dec 2, 2025
        r'(\d{1,2}[/-]\d{1,2}[/-]\d{4})',  # 12/02/2025
    ]
    
    # Time patterns (start-end time range)
    time_patterns = [
        r'(\d{1,2}:\d{2}\s*[-–—to]+\s*\d{1,2}:\d{2})',  # 14:00 - 14:45
    ]
    
    # Extract all dates and times from answer
    found_dates = []
    found_times = []
    
    for pattern in date_patterns:
        for match in re.finditer(pattern, answer_lower):
            found_dates.append((match.start(), match.group(1)))  # (position, date string)
    
    for pattern in time_patterns:
        for match in re.finditer(pattern, answer_lower):
            found_times.append((match.start(), match.group(1)))  # (position, time string)
    
    # Match dates and times into pairs
    # Match times that appear after each date with that date
    date_time_pairs = []
    for i, (date_pos, date_str) in enumerate(found_dates):
        # Range until next date
        next_date_pos = found_dates[i+1][0] if i+1 < len(found_dates) else len(answer_lower)
        # Match all times within this range
        for time_pos, time_str in found_times:
            if date_pos < time_pos < next_date_pos:
                date_time_pairs.append((date_str, time_str))
    
    # If no structured pairs found, try all date-time combinations (less strict matching)
    if not date_time_pairs and found_dates and found_times:
        for date_str, _ in found_dates:
            for _, time_str in found_times:
                date_time_pairs.append((date_str, time_str))
    
    # ============================================================
    # Step 2: Normalize date/time formats
    # ============================================================
    # Convert various date/time formats to canonical format (YYYY-MM-DD, HH:MM-HH:MM)
    
    def normalize_date(date_str):
        """Convert various date formats to YYYY-MM-DD"""
        import datetime
        formats = [
            "%Y-%m-%d",  # 2025-12-02
            "%B %d, %Y",  # December 2, 2025
            "%b %d, %Y",  # Dec 2, 2025
            "%m/%d/%Y",  # 12/02/2025
            "%d/%m/%Y",  # 02/12/2025
            "%m-%d-%Y",  # 12-02-2025
        ]
        for fmt in formats:
            try:
                dt = datetime.datetime.strptime(date_str, fmt)
                return dt.strftime("%Y-%m-%d")
            except:
                continue
        return None
    
    def normalize_time(time_str):
        """Convert various time formats to HH:MM-HH:MM"""
        time_str_clean = re.sub(r'\s+', '', time_str.lower())
        # Extract both start and end times
        times = re.findall(r'(\d{1,2}):(\d{2})', time_str_clean)
        if len(times) >= 2:
            start_h, start_m = times[0]
            end_h, end_m = times[1]
            return f"{int(start_h):02d}:{start_m}-{int(end_h):02d}:{end_m}"
        return None
    
    # Normalize agent's mentioned slots into a set
    agent_slots = set()
    for date_str, time_str in date_time_pairs:
        norm_date = normalize_date(date_str)
        norm_time = normalize_time(time_str)
        if norm_date and norm_time:
            agent_slots.add((norm_date, norm_time))
    
    # Convert canonical slots to the same format
    canonical_slots = {(slot["date"], slot["slot"]) for slot in slots}
    
    # ============================================================
    # Step 3: Calculate F1 score
    # ============================================================
    matched_canonical = len(agent_slots & canonical_slots)  # Number of matching canonical slots
    total_agent_slots = len(agent_slots)  # Total number of slots proposed by agent
    total_canonical = len(canonical_slots)  # Total number of canonical slots
    
    # Edge cases
    if total_canonical == 0:
        # No canonical slots to match - return 0 if agent proposed anything, 1.0 if nothing
        return 1.0 if total_agent_slots == 0 else 0.0
    
    if matched_canonical == 0:
        # No matches at all
        return 0.0
    
    # Calculate Precision and Recall
    precision = matched_canonical / total_agent_slots if total_agent_slots > 0 else 0.0
    recall = matched_canonical / total_canonical
    
    # Calculate F1 score (harmonic mean of Precision and Recall)
    if precision + recall == 0:
        return 0.0
    
    f1_score = 2 * (precision * recall) / (precision + recall)
    return f1_score


def evaluate_task(
    task: Task,
    agent_model: str = "gpt-4o-mini",
    generate_data: bool = True,
    tool_context_mode: str = "detailed"
) -> Dict[str, Any]:
    """
    [Core function] Evaluate a single task
    
    Overall flow:
        1. Generate or load source data
        2. Validate data consistency
        3. Run agent (LLM uses tools to generate answer)
        4. Score the answer
        5. Save results
    
    Args:
        task: Task definition to evaluate
        agent_model: LLM model name to use
        generate_data: Whether to generate data if not present
        tool_context_mode: "minimal" or "detailed" (tool description detail level)
        
    Returns:
        Evaluation result dictionary:
        {
            "task_id": ...,
            "agent_model": ...,
            "agent_result": {...},  # final_answer, rationale, tool_calls
            "scores": {
                "answer_requirements_satisfaction": 0.0~1.0
            },
            ...
        }
    """
    # ============================================================
    # Step 1: Generate or load source data
    # ============================================================
    task_data_dir = LOGS_DIR / task.id / "data"
    data_gen_token_usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
    if generate_data:
        # Generate data (call generate_source_data from data_gen.py)
        source_data, data_gen_token_usage = generate_source_data(task, task_data_dir)
    else:
        # Load existing data
        source_data = {}
        source_files = {
            "calendar": task_data_dir / "calendar.json",
            "slack": task_data_dir / "slack.json",
            "jira": task_data_dir / "jira.json",
            "drive": task_data_dir / "drive.json",
            "gmail": task_data_dir / "gmail.json"
        }
        for source_name, file_path in source_files.items():
            if file_path.exists():
                source_data[source_name] = file_path

    # ============================================================
    # Step 2: Run agent
    # ============================================================
    # LLM uses tools to solve the task and generate an answer
    run_log_path = LOGS_DIR / task.id / f"agent-{agent_model}_{tool_context_mode}_run.json"
    agent_result = run_task(task, source_data, agent_model, run_log_path, tool_context_mode=tool_context_mode)

    # ============================================================
    # Step 3: Score the answer
    # ============================================================
    agent_answer_text = agent_result.get("final_answer", "")
    
    # Calculate score by task category
    if task.is_planning():
        answer_score = score_planning_answer(task, agent_answer_text)
    else:
        answer_score = 0.0  # TODO: Implement other categories

    # Extract token usage from agent_result
    agent_token_usage = agent_result.get("token_usage", {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0})
    
    # Calculate total token usage
    total_token_usage = {
        "data_generation": data_gen_token_usage,
        "agent_execution": agent_token_usage,
        "total": {
            "prompt_tokens": data_gen_token_usage["prompt_tokens"] + agent_token_usage["prompt_tokens"],
            "completion_tokens": data_gen_token_usage["completion_tokens"] + agent_token_usage["completion_tokens"],
            "total_tokens": data_gen_token_usage["total_tokens"] + agent_token_usage["total_tokens"]
        }
    }
    
    result = {
        "task_id": task.id,
        "agent_model": agent_model,
        "task_category": task.category,
        "metadata": task.metadata,
        "canonical_answer": task.canonical_answer,
        "agent_result": agent_result,
        "scores": {
            "answer_requirements_satisfaction": answer_score
        },
        "token_usage": total_token_usage
    }

    # Save evaluation result
    eval_log_path = LOGS_DIR / task.id / f"agent-{agent_model}_{tool_context_mode}_eval.json"
    eval_log_path.parent.mkdir(parents=True, exist_ok=True)
    with open(eval_log_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2)

    return result


def evaluate_all_tasks(
    generate_data: bool = True,
    tool_context_modes: List[str] = None
) -> Dict[str, Any]:
    """
    [Main function] Orchestration function to evaluate all tasks
    
    Overall flow:
        - Evaluate all combinations of task × agent model × tool_context_mode
        - Call evaluate_task() for each combination
        - Print progress
    
    Args:
        generate_data: Whether to generate data if not present
        tool_context_modes: List of tool context modes to evaluate (None for ["minimal", "detailed"])
        
    Returns:
        Dictionary of evaluation results for all task-model-mode combinations
        key: "{task_id}__{model}__{mode}"
    """
    # Always load all models from model_config.json
    try:
        with open(MODEL_CONFIG_PATH, 'r', encoding='utf-8') as f:
            model_config = json.load(f)
            agent_models = model_config.get("agent_models", ["gpt-4o-mini"])
    except Exception:
        agent_models = ["gpt-4o-mini"]

    if tool_context_modes is None:
        tool_context_modes = ["minimal", "detailed"]
    
    tasks = load_all_tasks()
    results = {}
    
    total_combinations = len(tasks) * len(agent_models) * len(tool_context_modes)
    current = 0

    for task_idx, task in enumerate(tasks, 1):
        for model in agent_models:
            for mode in tool_context_modes:
                current += 1
                key = f"{task.id}__{model}__{mode}"
                print(f"[{mode}] Task {task_idx}/{len(tasks)} ({model}): {task.id}...", end=" ", flush=True)
                try:
                    results[key] = evaluate_task(task, model, generate_data, tool_context_mode=mode)
                    print(f"✓")
                except Exception as e:
                    print(f"⚠️  Error: {e}")
                    # Skip this mode and continue with next
                    continue

    return results

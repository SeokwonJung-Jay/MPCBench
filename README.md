# MPCBench: Multi-Source Workplace Benchmark

A benchmark for evaluating LLM agents that use multiple workplace data sources (Slack, Gmail, Calendar, Contacts, Jira, Drive) to complete tasks.

## Overview

MPCBench consists of two main pipelines:

1. **Data Generation Pipeline**: Creates realistic workplace scenarios and source data
2. **Evaluation Pipeline**: Tests LLM agents on tasks and evaluates their performance

---

## Human vs LLM Responsibilities

### ðŸ‘¤ Human-Authored Content

These files are manually created by humans:

- **`scenarios/{scenario_id}.json`**: High-level scenario specification
  - People, projects, abstract sub-scenarios
  - Configuration: `noise_level`, `depth`, `global_settings`
  
- **`tasks/{task_type}/{task_id}.json`**: Task definitions
  - `task_id`, `user_prompt`
  - `answer_requirements`: Specification of what a good answer should do
  - `expected_tool_sequence` (optional): Expected tool usage pattern

- **`prompt_config.json`**: System prompts for LLMs
  - Agent prompts
  - Judge evaluation prompts
  - Generator prompts

- **`model_config.json`**: Model configuration
  - `agent_models`: List of models to test
  - `judge_models`: List of models to use as judges
  - `data_generation_model`: Model for data generation

### ðŸ¤– LLM-Generated Content

These files are automatically generated by LLMs:

- **`data/{scenario_id}_world_state.json`**: Expanded world state
- **`data/{scenario_id}_{source}.json`**: Source-specific data files
  - `{source}` = `slack`, `gmail`, `calendar`, `contacts`, `jira`, `drive`

- **`evaluation/logs/{task_id}__agent-{model}_run.json`**: Agent execution logs
- **`evaluation/logs/{task_id}__agent-{model}__judge-{judge}_judge_input.json`**: Judge input
- **`evaluation/logs/{task_id}__agent-{model}__judge-{judge}_judge_result.json`**: Judge scores

---

## Data Generation Pipeline

### Step 1: Scenario â†’ World State

**Script**: `generator/world_state_generator.py`

**Input**:
- `scenarios/{scenario_id}.json` (human-authored)
- `prompt_config.json` â†’ `generator.world_state.system_prompt`
- `model_config.json` â†’ `data_generation_model`

**Output**:
- `data/{scenario_id}_world_state.json`

**Logic**:
1. Loads scenario JSON with abstract `sub_scenarios`
2. Calls LLM with system prompt to:
   - Expand abstract sub-scenarios into concrete `sub_scenarios_expanded` with:
     - Concrete ISO timestamps
     - Specific participant IDs/emails
     - Detailed event descriptions
   - Generate `noise_scenarios` based on `noise_level` parameter
   - Preserve all metadata (people, projects, global_settings)
3. Validates and saves world_state JSON

**Execution**:
```bash
python3 -m generator.world_state_generator --scenario-id scenario_A
```

**Key Fields in Output**:
- `people`: List of people with id, name, email, team, role
- `projects`: Projects with milestones and jira_seeds
- `sub_scenarios_expanded`: Core scenario events (concrete timestamps, participants)
- `noise_scenarios`: Unrelated workplace events (scales with noise_level)
- `global_settings`: Timezone, workday hours

---

### Step 2: World State â†’ Source Data

**Script**: `generator/run_generation.py`

**Input**:
- `data/{scenario_id}_world_state.json` (from Step 1)
- `prompt_config.json` â†’ `generator.{source}.system_prompt` for each source
- `model_config.json` â†’ `data_generation_model`
- `schemas/{source}_schema.json` for validation

**Output**:
- `data/{scenario_id}_slack.json`
- `data/{scenario_id}_gmail.json`
- `data/{scenario_id}_calendar.json`
- `data/{scenario_id}_contacts.json`
- `data/{scenario_id}_jira.json`
- `data/{scenario_id}_drive.json`

**Logic**:
1. Loads world_state JSON
2. For each source (Slack, Gmail, Calendar, Contacts, Jira, Drive):
   - Calls corresponding generator function:
     - `generate_slack_data(world_state, model)`
     - `generate_gmail_data(world_state, model)`
     - `generate_calendar_data(world_state, model)`
     - `generate_contacts_data(world_state, model)`
     - `generate_jira_data(world_state, model)`
     - `generate_drive_data(world_state, model)`
   - Each generator:
     - Builds prompt from world_state (sub_scenarios_expanded, noise_scenarios, people, etc.)
     - Calls LLM with source-specific system prompt
     - LLM generates data conforming to source schema
     - Validates against schema
     - Returns structured JSON
3. Saves each source data file to `data/` directory

**Execution**:
```bash
python3 -m generator.run_generation --scenario-id scenario_A
```

**Source Data Structure**:
- **Slack**: `workspace_name`, `channels[]`, `messages[]`, `users[]`
- **Gmail**: `threads[]` with `messages[]`
- **Calendar**: `events[]`, `calendars[]`
- **Contacts**: `contacts[]` with name, email
- **Jira**: `projects[]`, `issues[]`
- **Drive**: `files[]` with name, content, modifiedTime

---

## Evaluation Pipeline

### Step 1: Task Execution (Agent)

**Script**: `evaluation/run_single_task.py`

**Input**:
- `tasks/{task_type}/{task_id}.json` (human-authored)
  - `task_id`, `user_prompt`, `answer_requirements`
- `data/{scenario_id}_*.json` (all source data files)
- `prompt_config.json` â†’ `evaluation.agent.system_message`
- `model_config.json` â†’ `agent_models` (or `--agent-model` argument)

**Output**:
- `evaluation/logs/{task_id}__agent-{model}_run.json`

**Logic**:
1. Loads task JSON
2. Initializes `ToolBackend`:
   - Loads all source data files from `data/` directory
   - Creates query interfaces for each source (Slack, Calendar, Contacts, etc.)
3. Calls `run_task_with_openai()`:
   - Sends agent system prompt to LLM
   - Enters tool-calling loop:
     - Agent sends tool call requests
     - `ToolBackend` executes queries against loaded source data
     - Tool results returned to agent
     - Agent processes results and may call more tools
     - Loop continues until agent provides final answer
   - Agent returns JSON: `{"final_answer": "...", "rationale": "..."}`
4. Builds run log with:
   - `task_id`, `task_type`, `user_prompt`
   - `tool_trace_steps`: Human-readable tool call summary
   - `raw_tool_calls`: Full tool call records with `tool_name`, `arguments`, `result`
   - `final_answer`, `rationale`
5. Saves run log to `evaluation/logs/`

**Execution**:
```bash
python3 -m evaluation.run_single_task \
  --task tasks/planning/planning_task.json \
  --scenario-id scenario_A \
  --agent-model gpt-4o-mini
```

**Key Fields in Output**:
- `tool_trace_steps`: `["Step 1: Slack.search_messages(...)", ...]`
- `raw_tool_calls`: `[{tool_name, arguments, result}, ...]`
- `final_answer`: Agent's final response to user
- `rationale`: Agent's explanation of its process

---

### Step 2: Judge Input Generation

**Script**: `evaluation/build_judge_input.py`

**Input**:
- `evaluation/logs/{task_id}__agent-{model}_run.json` (from Step 1)
- `tasks/{task_type}/{task_id}.json` (to extract `answer_requirements`)

**Output**:
- `evaluation/logs/{task_id}__agent-{model}__judge-{judge}_judge_input.json`

**Logic**:
1. Loads run log JSON
2. Loads task JSON to extract `answer_requirements`
3. Infers `task_type` from task or task_id
4. Builds judge input dict:
   - `task_id`, `task_type`, `user_prompt` (from run log)
   - `answer_requirements` (from task JSON)
   - `tool_trace_steps` (from run log, verbatim)
   - `raw_tool_calls` (from run log, verbatim - full tool queries and results)
   - `tool_trace` (transformed from raw_tool_calls with step indices)
   - `final_answer`, `rationale` (from run log)
5. Saves judge input JSON

**Execution**:
```bash
python3 -m evaluation.build_judge_input \
  --log evaluation/logs/companyA_planning_v1__agent-gpt-4o_run.json \
  --output evaluation/logs/companyA_planning_v1__agent-gpt-4o__judge-gpt-5.1_judge_input.json
```

**Key Fields in Output**:
- `raw_tool_calls`: Full tool execution records (primary source of truth)
- `tool_trace`: Structured view with step indices
- `answer_requirements`: Specification for correctness evaluation

---

### Step 3: Judge Evaluation

**Script**: `evaluation/judge_runner.py`

**Input**:
- `evaluation/logs/{task_id}__agent-{model}__judge-{judge}_judge_input.json` (from Step 2)
- `prompt_config.json` â†’ `evaluation.judge.system_prompt`
- `model_config.json` â†’ `judge_models` (or `--model` argument)

**Output**:
- `evaluation/logs/{task_id}__agent-{model}__judge-{judge}_judge_result.json`

**Logic**:
1. Loads judge input JSON
2. Loads judge system prompt from `prompt_config.json`
3. Serializes entire judge input to JSON string
4. Calls judge LLM with:
   - System message: Judge evaluation prompt
   - User message: Judge input JSON (stringified)
   - **No tool calling** - judge is a pure offline evaluator
5. Judge LLM analyzes:
   - `answer_requirements_satisfaction`: Does final_answer meet requirements?
   - `source_grounded_reasoning`: Is rationale/answer faithful to tool results?
6. Judge returns JSON with scores (0-5) and justifications
7. Saves judge result JSON

**Execution**:
```bash
python3 -m evaluation.judge_runner \
  --input evaluation/logs/companyA_planning_v1__agent-gpt-4o__judge-gpt-5.1_judge_input.json \
  --model gpt-5.1
```

**Key Fields in Output**:
```json
{
  "answer_requirements_satisfaction": {
    "score": 3,
    "justification": "..."
  },
  "source_grounded_reasoning": {
    "score": 4,
    "justification": "..."
  }
}
```

---

### End-to-End Evaluation

**Script**: `evaluation/run_task_and_judge.py`

**Input**:
- `tasks/{task_type}/{task_id}.json`
- `data/{scenario_id}_*.json` (source data)
- `model_config.json` â†’ `agent_models`, `judge_models`

**Output**:
- All files from Steps 1-3 for each agent Ã— judge combination

**Logic**:
1. For each `agent_model` in `agent_models`:
   - Runs `run_single_task()` â†’ generates `*_run.json`
   - For each `judge_model` in `judge_models`:
     - Runs `build_judge_input_from_log_and_save()` â†’ generates `*_judge_input.json`
     - Runs `run_judge()` â†’ generates `*_judge_result.json`
2. Prints summary of all scores
3. Returns dict of scores keyed by `agent={model}__judge={model}`

**Execution**:
```bash
python3 -m evaluation.run_task_and_judge \
  --task tasks/planning/planning_task.json \
  --scenario-id scenario_A
```

---

## Complete Workflow Example

### 1. Generate Data

```bash
# Step 1: Create world state
python3 -m generator.world_state_generator --scenario-id scenario_A

# Step 2: Generate source data
python3 -m generator.run_generation --scenario-id scenario_A
```

### 2. Run Evaluation

```bash
# Single task execution
python3 -m evaluation.run_single_task \
  --task tasks/planning/planning_task.json \
  --scenario-id scenario_A \
  --agent-model gpt-4o-mini

# End-to-end (task + judge)
python3 -m evaluation.run_task_and_judge \
  --task tasks/planning/planning_task.json \
  --scenario-id scenario_A
```

---

## File Structure

```
MPCBench/
â”œâ”€â”€ scenarios/              # Human-authored scenario specs
â”‚   â””â”€â”€ scenario_A.json
â”œâ”€â”€ tasks/                  # Human-authored task definitions
â”‚   â”œâ”€â”€ planning/
â”‚   â”œâ”€â”€ email_reply/
â”‚   â””â”€â”€ document_generation/
â”œâ”€â”€ data/                   # LLM-generated data files
â”‚   â”œâ”€â”€ scenario_A_world_state.json
â”‚   â”œâ”€â”€ scenario_A_slack.json
â”‚   â”œâ”€â”€ scenario_A_gmail.json
â”‚   â”œâ”€â”€ scenario_A_calendar.json
â”‚   â”œâ”€â”€ scenario_A_contacts.json
â”‚   â”œâ”€â”€ scenario_A_jira.json
â”‚   â””â”€â”€ scenario_A_drive.json
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ logs/              # Evaluation logs
â”‚   â”‚   â”œâ”€â”€ *_run.json              # Agent execution logs
â”‚   â”‚   â”œâ”€â”€ *_judge_input.json      # Judge input
â”‚   â”‚   â”œâ”€â”€ *_judge_result.json     # Judge scores
â”‚   â”‚   â””â”€â”€ llm/                    # LLM call logs
â”‚   â”œâ”€â”€ run_single_task.py          # Step 1: Agent execution
â”‚   â”œâ”€â”€ build_judge_input.py        # Step 2: Judge input builder
â”‚   â”œâ”€â”€ judge_runner.py             # Step 3: Judge evaluation
â”‚   â””â”€â”€ run_task_and_judge.py       # End-to-end orchestrator
â”œâ”€â”€ generator/
â”‚   â”œâ”€â”€ world_state_generator.py    # Scenario â†’ World State
â”‚   â””â”€â”€ run_generation.py           # World State â†’ Source Data
â”œâ”€â”€ schemas/                # JSON schemas for validation
â”œâ”€â”€ prompt_config.json      # LLM system prompts
â””â”€â”€ model_config.json       # Model configuration
```

---

## Configuration

### `model_config.json`

```json
{
  "agent_models": ["gpt-4o-mini", "gpt-4o", "gpt-5.1"],
  "judge_models": ["gpt-5.1"],
  "data_generation_model": "gpt-4o"
}
```

### `prompt_config.json`

Contains system prompts for:
- `evaluation.agent.system_message`: Agent instructions
- `evaluation.judge.system_prompt`: Judge evaluation rubric
- `generator.world_state.system_prompt`: World state generation
- `generator.{source}.system_prompt`: Source data generation

---

## Key Design Principles

1. **Separation of Concerns**:
   - Data generation uses LLMs to create realistic scenarios
   - Evaluation uses recorded tool results (no re-execution)

2. **Judge as Offline Evaluator**:
   - Judge never calls tools or ToolBackend
   - Judge only sees recorded `raw_tool_calls` with full results
   - Enables consistent, reproducible evaluation

3. **Full Tool Trace Preservation**:
   - `raw_tool_calls`: Complete tool execution records
   - `tool_trace`: Structured view with step indices
   - Judge can verify factual consistency against actual tool outputs

4. **Two-Axis Evaluation**:
   - `answer_requirements_satisfaction`: Correctness relative to requirements
   - `source_grounded_reasoning`: Faithfulness to tool results

---

## Requirements

- Python 3.8+
- OpenAI API key (set `OPENAI_API_KEY` environment variable)
- Dependencies: `openai`, `json`, `pathlib`

---

## License

[Add license information]
